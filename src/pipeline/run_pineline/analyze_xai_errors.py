"""
XAI Error Analysis Script
Analyzes XAI explanations on 300 samples from dataset + 100 custom samples
to identify systematic errors and patterns.

Run: python scripts/analyze_xai_errors.py
Output: results/xai_error_analysis.json
"""

import sys
from pathlib import Path
import json
import random
from collections import defaultdict
from datetime import datetime

from transformers.utils import logging as hf_logging
hf_logging.set_verbosity_error()

# Add project root
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.pipeline.fact_checking.xai_phobert import load_xai_model
from datasets import load_dataset


# ============ Custom test samples (outside dataset) ============
CUSTOM_SAMPLES = [
    # REFUTES cases - clear contradiction
    {
        "claim": "Viá»‡t Nam thua Myanmar á»Ÿ lÆ°á»£t cuá»‘i, qua Ä‘Ã³ khÃ´ng thá»ƒ Ä‘i tiáº¿p báº£ng B SEA Games 33.",
        "evidence": "Viá»‡t Nam tháº¯ng Myanmar 2-0 á»Ÿ lÆ°á»£t cuá»‘i, qua Ä‘Ã³ Ä‘i tiáº¿p vá»›i tÆ° cÃ¡ch Ä‘á»™i nháº¥t báº£ng B SEA Games 33.",
        "expected": "REFUTES",
        "note": "thua vs tháº¯ng, khÃ´ng thá»ƒ vs Ä‘i tiáº¿p"
    },
    {
        "claim": "Tá»•ng thá»‘ng Biden tá»« chá»©c vÃ o thÃ¡ng 1 nÄƒm 2024.",
        "evidence": "Tá»•ng thá»‘ng Biden tiáº¿p tá»¥c nhiá»‡m ká»³ vÃ  khÃ´ng cÃ³ thÃ´ng bÃ¡o tá»« chá»©c nÃ o trong nÄƒm 2024.",
        "expected": "REFUTES",
        "note": "tá»« chá»©c vs tiáº¿p tá»¥c nhiá»‡m ká»³"
    },
    {
        "claim": "Apple ngá»«ng sáº£n xuáº¥t iPhone tá»« nÄƒm 2023.",
        "evidence": "Apple tiáº¿p tá»¥c ra máº¯t iPhone 15 vÃ o thÃ¡ng 9 nÄƒm 2023 vá»›i doanh sá»‘ ká»· lá»¥c.",
        "expected": "REFUTES",
        "note": "ngá»«ng sáº£n xuáº¥t vs tiáº¿p tá»¥c ra máº¯t"
    },
    {
        "claim": "Viá»‡t Nam khÃ´ng tham gia ASEAN.",
        "evidence": "Viá»‡t Nam lÃ  thÃ nh viÃªn chÃ­nh thá»©c cá»§a ASEAN tá»« nÄƒm 1995.",
        "expected": "REFUTES",
        "note": "khÃ´ng tham gia vs thÃ nh viÃªn chÃ­nh thá»©c"
    },
    {
        "claim": "HÃ  Ná»™i lÃ  thÃ nh phá»‘ lá»›n nháº¥t Viá»‡t Nam vá» diá»‡n tÃ­ch.",
        "evidence": "ThÃ nh phá»‘ Há»“ ChÃ­ Minh cÃ³ diá»‡n tÃ­ch 2.095 kmÂ², trong khi HÃ  Ná»™i cÃ³ diá»‡n tÃ­ch 3.358,6 kmÂ².",
        "expected": "SUPPORTS",
        "note": "HÃ  Ná»™i lá»›n hÆ¡n vá» diá»‡n tÃ­ch"
    },
    # SUPPORTS cases
    {
        "claim": "SÃ´ng Mekong cháº£y qua Viá»‡t Nam.",
        "evidence": "SÃ´ng Mekong báº¯t nguá»“n tá»« TÃ¢y Táº¡ng, cháº£y qua Trung Quá»‘c, Myanmar, LÃ o, ThÃ¡i Lan, Campuchia vÃ  Ä‘á»• ra biá»ƒn táº¡i Viá»‡t Nam.",
        "expected": "SUPPORTS",
        "note": "clear support"
    },
    {
        "claim": "Phá»Ÿ lÃ  mÃ³n Äƒn truyá»n thá»‘ng cá»§a Viá»‡t Nam.",
        "evidence": "Phá»Ÿ lÃ  má»™t trong nhá»¯ng mÃ³n Äƒn Ä‘áº·c trÆ°ng nháº¥t cá»§a áº©m thá»±c Viá»‡t Nam, Ä‘Æ°á»£c UNESCO cÃ´ng nháº­n lÃ  di sáº£n vÄƒn hÃ³a phi váº­t thá»ƒ.",
        "expected": "SUPPORTS",
        "note": "clear support"
    },
    # NEI cases
    {
        "claim": "NÄƒm 2025 sáº½ cÃ³ bÃ£o lá»›n Ä‘á»• bá»™ vÃ o Viá»‡t Nam.",
        "evidence": "Theo thá»‘ng kÃª, trung bÃ¬nh má»—i nÄƒm cÃ³ 5-6 cÆ¡n bÃ£o áº£nh hÆ°á»Ÿng Ä‘áº¿n Viá»‡t Nam.",
        "expected": "NEI",
        "note": "prediction vs historical data"
    },
    {
        "claim": "GiÃ¡ vÃ ng sáº½ tÄƒng trong nÄƒm tá»›i.",
        "evidence": "GiÃ¡ vÃ ng nÄƒm nay biáº¿n Ä‘á»™ng máº¡nh vá»›i nhiá»u phiÃªn tÄƒng giáº£m báº¥t thÆ°á»ng.",
        "expected": "NEI",
        "note": "future prediction"
    },
    {
        "claim": "Äá»™i tuyá»ƒn Viá»‡t Nam vÃ´ Ä‘á»‹ch AFF Cup 2024.",
        "evidence": "AFF Cup 2024 sáº½ diá»…n ra vÃ o cuá»‘i nÄƒm vá»›i sá»± tham gia cá»§a cÃ¡c Ä‘á»™i tuyá»ƒn ÄÃ´ng Nam Ã.",
        "expected": "NEI",
        "note": "result not mentioned"
    },
    # Edge cases - numbers
    {
        "claim": "GDP Viá»‡t Nam nÄƒm 2023 lÃ  500 tá»· USD.",
        "evidence": "GDP Viá»‡t Nam nÄƒm 2023 Ä‘áº¡t khoáº£ng 430 tá»· USD.",
        "expected": "REFUTES",
        "note": "number mismatch: 500 vs 430"
    },
    {
        "claim": "Viá»‡t Nam cÃ³ 63 tá»‰nh thÃ nh.",
        "evidence": "Viá»‡t Nam cÃ³ 63 Ä‘Æ¡n vá»‹ hÃ nh chÃ­nh cáº¥p tá»‰nh, bao gá»“m 5 thÃ nh phá»‘ trá»±c thuá»™c trung Æ°Æ¡ng vÃ  58 tá»‰nh.",
        "expected": "SUPPORTS",
        "note": "number match"
    },
    # Edge cases - negation
    {
        "claim": "Viá»‡t Nam khÃ´ng cÃ³ biÃªn giá»›i vá»›i Trung Quá»‘c.",
        "evidence": "Viá»‡t Nam cÃ³ Ä‘Æ°á»ng biÃªn giá»›i dÃ i 1.281 km vá»›i Trung Quá»‘c á»Ÿ phÃ­a Báº¯c.",
        "expected": "REFUTES",
        "note": "negation contradiction"
    },
    # Edge cases - time
    {
        "claim": "World Cup 2022 diá»…n ra táº¡i Qatar vÃ o mÃ¹a hÃ¨.",
        "evidence": "World Cup 2022 Ä‘Æ°á»£c tá»• chá»©c táº¡i Qatar tá»« 21/11 Ä‘áº¿n 18/12/2022, láº§n Ä‘áº§u tiÃªn diá»…n ra vÃ o mÃ¹a Ä‘Ã´ng.",
        "expected": "REFUTES",
        "note": "mÃ¹a hÃ¨ vs mÃ¹a Ä‘Ã´ng"
    },
    # Semantic similarity
    {
        "claim": "BÃ¡c Há»“ sinh nÄƒm 1890.",
        "evidence": "Chá»§ tá»‹ch Há»“ ChÃ­ Minh sinh ngÃ y 19 thÃ¡ng 5 nÄƒm 1890 táº¡i lÃ ng Kim LiÃªn, huyá»‡n Nam ÄÃ n, tá»‰nh Nghá»‡ An.",
        "expected": "SUPPORTS",
        "note": "BÃ¡c Há»“ = Chá»§ tá»‹ch Há»“ ChÃ­ Minh"
    },
]


def load_vifactcheck_samples(n_samples: int = 300) -> list:
    """Load random samples from ViFactCheck dataset."""
    print(f"ðŸ“¥ Loading {n_samples} samples from ViFactCheck...")
    
    try:
        dataset = load_dataset("tranthaihoa/vifactcheck", split="test")
        
        # Random sample
        indices = random.sample(range(len(dataset)), min(n_samples, len(dataset)))
        
        # Label mapping: 0=SUPPORTS, 1=REFUTES, 2=NEI
        label_map = {0: "SUPPORTS", 1: "REFUTES", 2: "NEI"}
        
        samples = []
        for idx in indices:
            item = dataset[idx]
            label_id = item["labels"]
            samples.append({
                "claim": item["Statement"],
                "evidence": item["Evidence"],
                "expected": label_map.get(label_id, "UNKNOWN"),
                "source": "vifactcheck",
                "idx": idx
            })
        
        print(f"   âœ… Loaded {len(samples)} samples")
        return samples
        
    except Exception as e:
        print(f"   âŒ Error loading dataset: {e}")
        return []


def analyze_xai_result(claim: str, evidence: str, xai_result: dict, expected: str) -> dict:
    """Analyze a single XAI result for errors."""
    errors = []
    warnings = []
    
    predicted = xai_result.get("relationship", "UNKNOWN")
    natural_explanation = xai_result.get("natural_explanation", "")
    claim_conflict = xai_result.get("claim_conflict_word", "")
    evidence_conflict = xai_result.get("evidence_conflict_word", "")
    
    # 1. Check prediction correctness
    prediction_correct = predicted == expected
    if not prediction_correct:
        errors.append(f"WRONG_PREDICTION: expected {expected}, got {predicted}")
    
    # 2. Check for underscore in display text (PyVi artifacts)
    if "_" in natural_explanation:
        errors.append(f"UNDERSCORE_IN_EXPLANATION: '{natural_explanation}'")
    
    # 4. For REFUTES - check conflict detection
    if predicted == "REFUTES":
        if not claim_conflict or not evidence_conflict:
            warnings.append("REFUTES_NO_CONFLICT_DETECTED")
        else:
            # Check if conflicts are meaningful
            if claim_conflict == evidence_conflict:
                errors.append(f"SAME_CONFLICT_WORDS: '{claim_conflict}'")
    
    # 5. Check natural explanation quality
    if not natural_explanation or len(natural_explanation) < 10:
        errors.append("EMPTY_OR_SHORT_EXPLANATION")
    
    # 7. Check for irrelevant conflict detection
    if predicted == "REFUTES" and claim_conflict and evidence_conflict:
        # Check if conflict words make semantic sense
        irrelevant_pairs = [
            # Words that shouldn't be marked as conflicts
            ("viá»‡t_nam", "viá»‡t_nam"),
            ("cá»§a", "cá»§a"),
            ("lÃ ", "lÃ "),
        ]
        if (claim_conflict.lower(), evidence_conflict.lower()) in irrelevant_pairs:
            errors.append(f"IRRELEVANT_CONFLICT: '{claim_conflict}' vs '{evidence_conflict}'")
    
    return {
        "prediction_correct": prediction_correct,
        "predicted": predicted,
        "expected": expected,
        "errors": errors,
        "warnings": warnings,
        "claim_conflict": claim_conflict,
        "evidence_conflict": evidence_conflict,
        "natural_explanation": natural_explanation
    }


def run_analysis():
    """Main analysis function."""
    print("=" * 60)
    print("ðŸ” XAI Error Analysis Script")
    print("=" * 60)
    
    # Load XAI model
    print("\nðŸ“¦ Loading PhoBERT XAI model...")
    model_path = project_root / "results/fact_checking/pyvi/checkpoints/best_model_pyvi.pt"
    
    try:
        xai = load_xai_model(str(model_path), device="cpu")
        print("   âœ… Model loaded")
    except Exception as e:
        print(f"   âŒ Error: {e}")
        return
    
    # Collect samples
    all_samples = []
    
    # 1. Dataset samples
    dataset_samples = load_vifactcheck_samples(300)
    all_samples.extend(dataset_samples)
    
    # 2. Custom samples
    print(f"\nðŸ“ Adding {len(CUSTOM_SAMPLES)} custom samples...")
    for sample in CUSTOM_SAMPLES:
        sample["source"] = "custom"
    all_samples.extend(CUSTOM_SAMPLES)
    
    print(f"\nðŸ“Š Total samples: {len(all_samples)}")
    
    # Run analysis
    print("\nðŸ”„ Analyzing XAI outputs...")
    results = []
    error_counts = defaultdict(int)
    warning_counts = defaultdict(int)
    
    for i, sample in enumerate(all_samples):
        if (i + 1) % 50 == 0:
            print(f"   Processing {i + 1}/{len(all_samples)}...")
        
        try:
            xai_result = xai.generate_xai(
                claim=sample["claim"],
                evidence=sample["evidence"]
            )
            
            analysis = analyze_xai_result(
                claim=sample["claim"],
                evidence=sample["evidence"],
                xai_result=xai_result,
                expected=sample["expected"]
            )
            
            analysis["sample"] = {
                "claim": sample["claim"][:100] + "..." if len(sample["claim"]) > 100 else sample["claim"],
                "evidence": sample["evidence"][:100] + "..." if len(sample["evidence"]) > 100 else sample["evidence"],
                "source": sample.get("source", "unknown"),
                "note": sample.get("note", "")
            }
            
            results.append(analysis)
            
            # Count errors and warnings
            for error in analysis["errors"]:
                error_type = error.split(":")[0]
                error_counts[error_type] += 1
            
            for warning in analysis["warnings"]:
                warning_type = warning.split(":")[0]
                warning_counts[warning_type] += 1
                
        except Exception as e:
            print(f"   âŒ Error on sample {i}: {e}")
            results.append({
                "sample": sample,
                "error": str(e),
                "errors": ["PROCESSING_ERROR"],
                "warnings": []
            })
            error_counts["PROCESSING_ERROR"] += 1
    
    # Summary statistics
    total = len(results)
    correct = sum(1 for r in results if r.get("prediction_correct", False))
    with_errors = sum(1 for r in results if len(r.get("errors", [])) > 0)
    with_warnings = sum(1 for r in results if len(r.get("warnings", [])) > 0)
    
    summary = {
        "timestamp": datetime.now().isoformat(),
        "total_samples": total,
        "dataset_samples": len(dataset_samples),
        "custom_samples": len(CUSTOM_SAMPLES),
        "prediction_accuracy": f"{correct / total * 100:.2f}%",
        "samples_with_errors": with_errors,
        "samples_with_warnings": with_warnings,
        "error_counts": dict(error_counts),
        "warning_counts": dict(warning_counts),
    }
    
    # Print summary
    print("\n" + "=" * 60)
    print("ðŸ“ˆ ANALYSIS SUMMARY")
    print("=" * 60)
    print(f"Total samples:        {total}")
    print(f"Prediction accuracy:  {summary['prediction_accuracy']}")
    print(f"Samples with errors:  {with_errors} ({with_errors/total*100:.1f}%)")
    print(f"Samples with warnings: {with_warnings} ({with_warnings/total*100:.1f}%)")
    
    print("\nðŸš¨ ERROR BREAKDOWN:")
    for error_type, count in sorted(error_counts.items(), key=lambda x: -x[1]):
        print(f"   {error_type}: {count} ({count/total*100:.1f}%)")
    
    print("\nâš ï¸ WARNING BREAKDOWN:")
    for warning_type, count in sorted(warning_counts.items(), key=lambda x: -x[1]):
        print(f"   {warning_type}: {count} ({count/total*100:.1f}%)")
    
    # Save results
    output_dir = project_root / "results"
    output_dir.mkdir(exist_ok=True)
    
    output_file = output_dir / "xai_error_analysis.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "summary": summary,
            "detailed_results": results[:50],  # Save first 50 for inspection
            "error_samples": [r for r in results if len(r.get("errors", [])) > 0][:30]
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸ’¾ Results saved to: {output_file}")
    
    # Print sample errors for inspection
    print("\n" + "=" * 60)
    print("ðŸ“‹ SAMPLE ERRORS (first 5)")
    print("=" * 60)
    
    error_samples = [r for r in results if len(r.get("errors", [])) > 0][:5]
    for i, sample in enumerate(error_samples, 1):
        print(f"\n--- Sample {i} ---")
        print(f"Claim: {sample['sample']['claim']}")
        print(f"Expected: {sample.get('expected', 'N/A')} | Got: {sample.get('predicted', 'N/A')}")
        print(f"Errors: {sample['errors']}")
        if sample.get('claim_conflict') or sample.get('evidence_conflict'):
            print(f"Conflicts: '{sample.get('claim_conflict', '')}' vs '{sample.get('evidence_conflict', '')}'")
        print(f"Explanation: {sample.get('natural_explanation', 'N/A')[:100]}")


if __name__ == "__main__":
    random.seed(42)  # Reproducibility
    run_analysis()
