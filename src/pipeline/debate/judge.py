"""
Judge - Synthesizes debate arguments thành final verdict.

Author: Lockdown
Date: Nov 10, 2025
"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
import logging
import json
import re
import ast

from .debator import DebateArgument, Evidence

logger = logging.getLogger(__name__)


@dataclass
class JudgeR2Anchor:
    """DEPRECATED: No longer used in 2-round system. Kept for backward compatibility."""
    verdict_r2: str = "NEI"
    key_factual_basis: str = ""
    unresolved_conflicts: str = ""
    confidence: float = 0.5


@dataclass
class FinalVerdict:
    """Final verdict từ Judge sau khi tổng hợp debate."""
    verdict: str  # SUPPORTS, REFUTES, NOT_ENOUGH_INFO (Debate verdict)
    confidence: float  # 0.0 - 1.0
    reasoning: str
    evidence_summary: str
    rounds_used: int
    debator_agreements: Dict[str, int]  # verdict -> count
    early_stopped: bool
    stop_reason: str
    mvp_agent: str = None  # Agent with most convincing argument (legacy)
    best_quote_from: str = None  # Agent with best evidence quote (new)
    decision_path: str = None  # CONSENSUS | MAJORITY | MINORITY_OVERRIDE | MODEL_TIEBREAK
    round_1_verdicts: Dict[str, Dict[str, Any]] = None  # {model_name: {verdict, confidence}} for inter-agent agreement
    all_rounds_verdicts: List[Dict[str, Dict[str, Any]]] = None  # All rounds data for metrics visualization
    r2_anchor: JudgeR2Anchor = None  # Memory Anchor from Round 2 (deprecated)
    # Hybrid Strategy fields (DOWN Framework, 2025)
    hybrid_verdict: str = None  # Final verdict after applying hybrid strategy
    hybrid_source: str = None  # MODEL_HIGH_CONF | DEBATE
    hybrid_threshold: float = None  # Threshold used for decision
    model_confidence_used: float = None  # Model confidence at decision time
    # XAI fields (Dec 2025) - Structured explanation
    xai_conflict_claim: str = None  # Conflict word in claim (for REFUTES)
    xai_conflict_evidence: str = None  # Conflict word in evidence (for REFUTES)
    xai_natural_explanation: str = None  # Natural language explanation in Vietnamese
    xai_dict: dict = None  # Full XAI dict (generated by orchestrator)
    # Metrics fields (Dec 2025) - For paper analysis
    consensus_round: int = None  # Round at which consensus was first reached (None if no consensus)
    total_tokens_used: int = None  # Total tokens consumed in debate
    tokens_per_round: List[int] = None  # Tokens used per round


class Judge:
    """
    Judge synthesizes arguments từ multiple debators.
    Makes final verdict based on debate history.
    """
    
    def __init__(self, model_config: Dict[str, Any], llm_client, generate_xai: bool = False):
        """
        Args:
            model_config: Config cho judge model
            llm_client: LLM client để gọi API
            generate_xai: If True, include XAI in prompt (for demo UI). Default False for experiments.
        """
        self.config = model_config
        self.llm_client = llm_client
        self.generate_xai = generate_xai

    # NOTE: summarize_round methods removed in 2-round system (Dec 2025)
    # Judge no longer provides mid-debate guidance

    # NOTE: generate_r2_anchor_async removed in 2-round system (Dec 2025)
    # No longer needed as R3 was removed

    def decide(
        self,
        claim: str,
        debate_history: List[List[DebateArgument]],
        early_stopped: bool = False,
        stop_reason: str = "",
        model_verdict: str = None,
        model_confidence: float = None,
        evidences: List[Evidence] = None,
        r2_anchor: JudgeR2Anchor = None  # DEPRECATED: kept for backward compatibility
    ) -> FinalVerdict:
        """
        Make final verdict dựa trên debate history (2-round system).
        Called ONCE after Round 2 to make final decision.
        
        Args:
            claim: Original claim
            debate_history: List of rounds (R1 + R2)
            early_stopped: Whether debate stopped early
            stop_reason: Reason for early stopping
            evidences: List of gold evidences (not used in prompt)
            r2_anchor: DEPRECATED - no longer used
            
        Returns:
            FinalVerdict
        """
        
        current_round = len(debate_history)
        is_final_round = current_round >= 2
        
        # Build prompt with appropriate inputs
        prompt = self._build_judge_prompt(
            claim=claim,
            debate_history=debate_history,
            model_verdict=model_verdict,
            model_confidence=model_confidence,
            evidences=evidences,
            r2_anchor=r2_anchor
        )
        
        try:
            # Call LLM
            response = self.llm_client.generate(
                model=self.config['model'],
                prompt=prompt,
                api_key=self.config['api_key'],
                base_url=self.config['base_url'],
                temperature=self.config.get('temperature', 0.3),
                max_tokens=self.config.get('max_tokens', 1000)
            )
            
            # Parse response
            verdict = self._parse_judge_response(
                response,
                claim,
                evidences,
                debate_history,
                early_stopped,
                stop_reason
            )
            
            logger.info(f"Judge final verdict: {verdict.verdict} (conf={verdict.confidence:.2f})")
            
            # NOTE: SAFETY OVERRIDE removed - using SLM-guided signal injection instead
            # The model signal is now part of the prompt context, letting debate decide naturally

            return verdict
            
        except Exception as e:
            logger.error(f"Judge error: {e}")
            # Fallback: majority voting
            return self._fallback_majority_vote(
                debate_history, 
                early_stopped, 
                stop_reason
            )
    
    # Vietnamese verdict mapping for Judge
    VERDICT_VI = {
        "SUPPORTED": "ĐỒNG TÌNH",
        "REFUTED": "BÁC BỎ",
        "NEI": "CHƯA ĐỦ THÔNG TIN",
        "NOT_ENOUGH_INFO": "CHƯA ĐỦ THÔNG TIN",
    }
    
    def _verdict_to_vi(self, verdict: str) -> str:
        """Convert verdict to Vietnamese."""
        return self.VERDICT_VI.get(verdict.upper(), verdict)
    
    # Agent name mapping for Judge
    AGENT_NAME_MAP = {
        "Truth Seeker A": "Grok",
        "Truth Seeker B": "Gemini",
        "Truth Seeker C": "GPT",
    }
    
    def _build_judge_prompt(
        self,
        claim: str,
        debate_history: List[List[DebateArgument]],
        model_verdict: str = None,
        model_confidence: float = None,
        evidences: List[Evidence] = None,
        r2_anchor: JudgeR2Anchor = None
    ) -> str:
        """Build evidence-grounded prompt for Judge."""
        
        # Clean claim
        claim_clean = claim.replace("_", " ").replace("  ", " ") if claim else claim
        
        # ===== BUILD PROMPT =====
        num_rounds = len(debate_history)
        # Role description - simplified for experiments, full for demo
        if self.generate_xai:
            role_desc = "You are the JUDGE - Vote Aggregator + XAI Formatter."
            job_desc = "Your job: Follow vote rules and provide structured explanation (XAI). You MUST follow vote rules. You are NOT allowed to change verdict except tie policy."
        else:
            role_desc = "You are the JUDGE - Vote Aggregator."
            job_desc = "Your job: Follow vote rules strictly. You MUST follow vote rules. You are NOT allowed to change verdict except tie policy."
        
        prompt = f"""**ROLE:** {role_desc}

**CONTEXT:**
- Round 1: Debators analyzed INDEPENDENTLY (no cross-examination)
- Round 2+: Debators cross-examined each other's arguments
- Total rounds in this debate: {num_rounds}
- {job_desc}

**CLAIM:** "{claim_clean}"
"""
        
        # ADD EVIDENCE to prompt - Judge can now verify quotes
        if evidences and len(evidences) > 0:
            prompt += "\n**Evidence:**\n"
            for i, ev in enumerate(evidences, 1):
                ev_text = ev.text.replace("_", " ").replace("  ", " ") if ev.text else ""
                if ev_text:
                    prompt += f"- {ev_text}\n"
        
        # R1 verdicts (independent/blind)
        r1_round = debate_history[0] if len(debate_history) > 0 else []
        if r1_round:
            prompt += "\n**ROUND 1 VERDICTS (Independent - No Model Info):**\n"
            for arg in r1_round:
                agent_name = self.AGENT_NAME_MAP.get(arg.role, arg.debator_name)
                prompt += f"- {agent_name}: {arg.verdict} ({arg.confidence:.0%})\n"
        
        # Final round verdicts (after cross-examination) - structured format
        prompt += f"\n**FINAL ROUND (Round {num_rounds}) VERDICTS:**\n"
        final_round = debate_history[-1] if debate_history else []
        for arg in final_round:
            agent_name = self.AGENT_NAME_MAP.get(arg.role, arg.debator_name)
            # Fix: Debator stores quote in key_points[0], not key_quote attribute (Dec 24, 2025)
            key_quote = arg.key_points[0] if arg.key_points else 'N/A'
            # Preserve context for validation - use head + tail if long
            if len(key_quote) > 240:
                key_quote = key_quote[:120] + " ... " + key_quote[-120:]
            elif len(key_quote) > 200:
                key_quote = key_quote  # Keep full quote if reasonable length
            prompt += f"- {agent_name}: **{arg.verdict}** ({arg.confidence:.0%})\n"
            prompt += f"  Quote: \"{key_quote}\"\n"
        
        # Flip analysis
        if r1_round and final_round:
            flips = []
            for r1_arg in r1_round:
                for r2_arg in final_round:
                    if r1_arg.debator_name == r2_arg.debator_name and r1_arg.verdict != r2_arg.verdict:
                        agent_name = self.AGENT_NAME_MAP.get(r1_arg.role, r1_arg.debator_name)
                        flips.append(f"{agent_name}: {r1_arg.verdict} → {r2_arg.verdict}")
            if flips:
                prompt += f"\n⚠️ **VERDICT CHANGES (R1→Final):** {', '.join(flips)}\n"
        
        # Count verdicts
        verdict_counts = {}
        for arg in final_round:
            verdict_counts[arg.verdict] = verdict_counts.get(arg.verdict, 0) + 1
        verdict_summary = ", ".join([f"{v}: {c}" for v, c in verdict_counts.items()])
        prompt += f"\n**FINAL VOTE COUNT:** {verdict_summary}\n"
        
        # Classifier signal - REMOVED in v2 (Pure Debate) to support Hybrid Routing
        # The Judge should rely ONLY on debate quality.
        # if model_verdict and model_confidence: ... (Removed)
        
        # Decision guidelines - pure debate logic
        prompt += """
---

**DECISION PROTOCOL:**

**STEP 0: VALIDATE QUOTES**
For each agent's quote:
- Does quote exist verbatim in evidence (normalize whitespace/punctuation)?
- Is quote direct to a key part of the claim?
- Is context correct?

1. **CONSENSUS (3/3 agree):**
   - Follow consensus ONLY if ≥2 agents have VALID+DIRECT quotes.
   - If consensus but <2 valid-direct quotes → return **NEI** (low-evidence consensus).

2. **MAJORITY (2/1):**
   - Follow majority verdict.
   - If no side has valid-direct quotes → **NEI**.

3. **SPLIT DECISION (1-1-1):** (Rare case: Support vs Refute vs NEI)
   - Automatically return **NEI** for safety (no consensus).
   - Provide structured analysis of disagreement.

**YOUR TASK:**
Summarize the debate and give your final verdict.
"""
        
        # Output format based on generate_xai flag
        if self.generate_xai:
            prompt += """
**OUTPUT (JSON):**
{
    "verdict": "SUPPORTED | REFUTED | NEI",
    "confidence": 0.0-1.0,
    "decision_path": "CONSENSUS | MAJORITY | SPLIT_DECISION",
    "explanation_vi": "Tóm tắt kết quả (2-3 câu tiếng Việt)",
    "xai": {
        "conflict_claim": "Nếu REFUTED: từ/cụm trong tuyên bố gây mâu thuẫn (ví dụ: 'thứ hai', 'giảm 5%')",
        "conflict_evidence": "Nếu REFUTED: từ/cụm trong bằng chứng mâu thuẫn với tuyên bố (ví dụ: 'đầu tiên', 'tăng 0.95%')",
        "natural_explanation_vi": "Giải thích 1 câu ngắn gọn bằng tiếng Việt"
    }
}

**XAI RULES:**
- conflict_claim/conflict_evidence: CHỈ điền nếu verdict là REFUTED, để trống "" nếu SUPPORTED/NEI
- conflict_claim: phải là từ/cụm từ XUẤT HIỆN NGUYÊN VĂN trong tuyên bố (verbatim substring)
- conflict_evidence: phải là từ/cụm từ XUẤT HIỆN NGUYÊN VĂN trong bằng chứng (verbatim substring)
- Tuyệt đối KHÔNG bịa thêm fact/entity/number ngoài tuyên bố/bằng chứng
- natural_explanation_vi: Giải thích theo format:
  - SUPPORTED: "Bằng chứng cung cấp thông tin phù hợp với tuyên bố."
  - REFUTED: "Tuyên bố nói '[conflict_claim]' nhưng Bằng chứng nói '[conflict_evidence]'. Hai thông tin này mâu thuẫn."
  - NEI: "Hiện tại, bằng chứng được cung cấp chưa đủ để kết luận tuyên bố đúng hay sai."""
        else:
            # Simplified output for experiments - no XAI, save tokens
            prompt += """
**OUTPUT (JSON):**
{
    "verdict": "SUPPORTED | REFUTED | NEI",
    "confidence": 0.0-1.0,
    "decision_path": "CONSENSUS | MAJORITY | SPLIT_DECISION",
    "reasoning": "Brief 1-sentence reasoning"
}
"""
        
        return prompt
    
    def _extract_all_rounds_verdicts(self, debate_history: List[List[DebateArgument]]) -> List[Dict[str, Dict[str, Any]]]:
        """Extract verdicts from all rounds for metrics visualization."""
        all_rounds = []
        for round_num, round_args in enumerate(debate_history, 1):
            round_data = {}
            for arg in round_args:
                round_data[arg.debator_name] = {
                    "verdict": arg.verdict,
                    "confidence": arg.confidence,
                    "reasoning": arg.reasoning,
                    "role": arg.role,
                    # XAI Interaction fields (Round 2+)
                    "agree_with": getattr(arg, 'agree_with', None),
                    "agree_reason": getattr(arg, 'agree_reason', None),
                    "disagree_with": getattr(arg, 'disagree_with', None),
                    "disagree_reason": getattr(arg, 'disagree_reason', None),
                    "changed": getattr(arg, 'changed', False),
                    "change_reason": getattr(arg, 'change_reason', None)
                }
            all_rounds.append(round_data)
        return all_rounds
    
    def _parse_judge_response(
        self,
        response: str,
        claim: str,
        evidences: List[Evidence],
        debate_history: List[List[DebateArgument]],
        early_stopped: bool,
        stop_reason: str
    ) -> FinalVerdict:
        """Parse judge response with robust error handling."""
        
        # Clean response
        cleaned = re.sub(r'```(?:json)?', '', response, flags=re.IGNORECASE).strip()
        
        # Helper: Find JSON with balanced braces (Borrowed from Debator)
        def find_json_with_balanced_braces(text):
            start = text.find('{')
            if start == -1:
                return None
            
            depth = 0
            for i, char in enumerate(text[start:], start):
                if char == '{':
                    depth += 1
                elif char == '}':
                    depth -= 1
                    if depth == 0:
                        return text[start:i+1]
            return None

        # Attempt to extract JSON
        json_str = find_json_with_balanced_braces(cleaned)
        
        if not json_str:
            # Fallback to regex if simple find fails
            json_match = re.search(r'\{.*\}', cleaned, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                logger.warning(f"Judge: Could not find JSON in response: {response[:100]}...")
                return self._fallback_majority_vote(debate_history, early_stopped, stop_reason)
        
        data = None
        # Attempt 1: Standard JSON parse
        try:
            data = json.loads(json_str)
        except json.JSONDecodeError:
            # Attempt 2: Python literal eval (Handles single quotes)
            try:
                data = ast.literal_eval(json_str)
                if not isinstance(data, dict):
                    raise ValueError("Parsed content is not a dictionary")
                logger.info("Judge: Recovered JSON using ast.literal_eval")
            except (ValueError, SyntaxError):
                # Attempt 3: Dirty Regex Extraction (Last Resort)
                logger.warning("Judge: JSON/AST parse failed. Attempting dirty regex extraction.")
                # Try both "verdict" and "final_verdict"
                verdict_match = re.search(r'"(?:final_)?verdict":\s*"?(SUPPORTED|REFUTED|NEI|NOT_ENOUGH_INFO)"?', cleaned, re.IGNORECASE)
                explanation_match = re.search(r'"explanation_vi":\s*"(.*?)"', cleaned, re.DOTALL)
                
                if verdict_match:
                    verdict_val = verdict_match.group(1).upper()
                    if verdict_val == "NOT_ENOUGH_INFO": verdict_val = "NEI"
                    explanation_val = explanation_match.group(1) if explanation_match else "Recovered from regex"
                    data = {
                        "verdict": verdict_val,
                        "confidence": 0.85,
                        "explanation_vi": explanation_val
                    }
                    logger.info(f"Judge: Emergency regex recovered verdict: {verdict_val}")
                else:
                    logger.warning("Judge: Failed to parse JSON even with fallback")
                    return self._fallback_majority_vote(debate_history, early_stopped, stop_reason)
        
        try:
            # Normalize verdict - handle both "verdict" and "final_verdict"
            raw_verdict = (data.get("verdict") or data.get("final_verdict") or "NEI").upper()
            confidence = float(data.get("confidence", 0.5))
            
            # Handle explanation_vi (XAI mode), reasoning (simple mode), or explanation (old)
            explanation_vi = data.get("explanation_vi", "")
            reasoning = data.get("reasoning", "")
            if explanation_vi:
                summary = explanation_vi
            elif reasoning:
                summary = reasoning
            else:
                explanation = data.get("explanation", {})
                summary = explanation.get("summary", "") if isinstance(explanation, dict) else str(explanation)
            key_evidence = ""
            if isinstance(data.get("explanation"), dict):
                key_evidence = data["explanation"].get("key_evidence", "")

            # Count agreements from final round
            final_round = debate_history[-1]
            agreements = {}
            for arg in final_round:
                agreements[arg.verdict] = agreements.get(arg.verdict, 0) + 1
            
            # 1-1-1 split → NEI (Safe fallback - Dec 23, 2025)
            total_debaters = len(final_round)
            if total_debaters == 3 and len(agreements) == 3:
                # Perfect 1-1-1 tie: all agents disagree
                logger.info("Judge: 1-1-1 split detected → returning NEI for safety")
                raw_verdict = "NEI"
                confidence = 0.5
                summary = "Ba agent hoàn toàn không đồng thuận (1-1-1 split), không đủ cơ sở để kết luận."
                data["decision_path"] = "SPLIT_DECISION"
            
            # Check if Judge result is unreliable (low confidence or empty reasoning)
            # AND debaters have strong consensus (≥ 2/3 agreement)
            max_agreement = max(agreements.values()) if agreements else 0
            consensus_ratio = max_agreement / total_debaters if total_debaters > 0 else 0
            
            judge_unreliable = (confidence < 0.6 or not summary.strip())
            strong_consensus = (consensus_ratio >= 0.66)  # At least 2/3 agree
            
            if judge_unreliable and strong_consensus:
                logger.warning(f"Judge unreliable (conf={confidence:.2f}, reasoning_empty={not summary.strip()}), "
                             f"using debater consensus ({max_agreement}/{total_debaters})")
                return self._fallback_majority_vote(debate_history, early_stopped, stop_reason)
            
            # Build round_1_verdicts (with full details for case study)
            round_1_verdicts = {}
            if len(debate_history) > 0:
                for arg in debate_history[0]:
                    round_1_verdicts[arg.debator_name] = {
                        "verdict": arg.verdict,
                        "confidence": arg.confidence,
                        "reasoning": arg.reasoning,
                        "role": arg.role
                    }
            
            # Extract all rounds verdicts for metrics
            all_rounds_verdicts = self._extract_all_rounds_verdicts(debate_history)
            
            # Extract XAI fields (Dec 2025)
            xai_data = data.get("xai", {})
            xai_conflict_claim = xai_data.get("conflict_claim", "") if isinstance(xai_data, dict) else ""
            xai_conflict_evidence = xai_data.get("conflict_evidence", "") if isinstance(xai_data, dict) else ""
            xai_natural_explanation = xai_data.get("natural_explanation_vi", "") if isinstance(xai_data, dict) else ""

            # Sanitize XAI fields to reduce hallucination
            verdict_upper = (raw_verdict or "").upper()
            if verdict_upper != "REFUTED":
                xai_conflict_claim = ""
                xai_conflict_evidence = ""
            else:
                claim_text = claim or ""
                evidence_text = ""
                if evidences:
                    try:
                        evidence_text = "\n".join([(getattr(e, "text", "") or "") for e in evidences])
                    except Exception:
                        evidence_text = ""

                cc = (xai_conflict_claim or "").strip()
                ce = (xai_conflict_evidence or "").strip()
                if not cc or not ce:
                    xai_conflict_claim = ""
                    xai_conflict_evidence = ""
                else:
                    claim_lower = claim_text.lower()
                    evidence_lower = evidence_text.lower()
                    if (cc.lower() not in claim_lower) or (evidence_text and (ce.lower() not in evidence_lower)):
                        xai_conflict_claim = ""
                        xai_conflict_evidence = ""

            if isinstance(xai_natural_explanation, str):
                xai_natural_explanation = xai_natural_explanation.replace("\n", " ").strip()
            
            return FinalVerdict(
                verdict=raw_verdict,
                confidence=confidence,
                reasoning=summary,
                evidence_summary=key_evidence,
                rounds_used=len(debate_history),
                debator_agreements=agreements,
                early_stopped=early_stopped,
                stop_reason=stop_reason,
                mvp_agent=data.get("mvp_agent") or data.get("best_quote_from", "Unknown"),
                best_quote_from=data.get("best_quote_from", "Unknown"),
                decision_path=data.get("decision_path", "UNKNOWN"),
                round_1_verdicts=round_1_verdicts,
                all_rounds_verdicts=all_rounds_verdicts,
                xai_conflict_claim=xai_conflict_claim,
                xai_conflict_evidence=xai_conflict_evidence,
                xai_natural_explanation=xai_natural_explanation
            )
            
        except Exception as e:
            logger.error(f"Judge structure error: {e}")
            return self._fallback_majority_vote(debate_history, early_stopped, stop_reason)
    
    def _fallback_majority_vote(
        self,
        debate_history: List[List[DebateArgument]],
        early_stopped: bool,
        stop_reason: str
    ) -> FinalVerdict:
        """Fallback to simple majority voting."""
        
        # Get final round arguments
        final_round = debate_history[-1]
        
        # Count verdicts
        verdict_counts = {}
        total_confidence = {}
        
        for arg in final_round:
            verdict_counts[arg.verdict] = verdict_counts.get(arg.verdict, 0) + 1
            total_confidence[arg.verdict] = total_confidence.get(arg.verdict, 0.0) + arg.confidence
        
        # Majority verdict
        majority_verdict = max(verdict_counts.items(), key=lambda x: x[1])[0]
        majority_count = verdict_counts[majority_verdict]
        avg_confidence = total_confidence[majority_verdict] / majority_count
        
        # Build reasoning
        reasoning = f"Majority vote: {majority_count}/{len(final_round)} debators chose {majority_verdict}."
        
        # Build round_1_verdicts for inter-agent agreement (with full details)
        round_1_verdicts = {}
        if len(debate_history) > 0:
            for arg in debate_history[0]:  # Round 1
                round_1_verdicts[arg.debator_name] = {
                    "verdict": arg.verdict,
                    "confidence": arg.confidence,
                    "reasoning": arg.reasoning,
                    "role": arg.role
                }
        
        # Extract all rounds verdicts for metrics
        all_rounds_verdicts = self._extract_all_rounds_verdicts(debate_history)
        
        return FinalVerdict(
            verdict=majority_verdict,
            confidence=avg_confidence,
            reasoning=reasoning,
            evidence_summary="Fallback to majority voting due to judge error.",
            rounds_used=len(debate_history),
            debator_agreements=verdict_counts,
            early_stopped=early_stopped,
            stop_reason=stop_reason,
            best_quote_from="Majority",
            decision_path="MAJORITY",
            round_1_verdicts=round_1_verdicts,
            all_rounds_verdicts=all_rounds_verdicts
        )
    
    async def decide_async(
        self,
        claim: str,
        debate_history: List[List[DebateArgument]],
        early_stopped: bool = False,
        stop_reason: str = "",
        model_verdict: str = None,
        model_confidence: float = None,
        evidences: List[Evidence] = None,
        r2_anchor: JudgeR2Anchor = None  # DEPRECATED: kept for backward compatibility
    ) -> FinalVerdict:
        """
        Async version - called ONCE after Round 2 to make final decision.
        
        Args:
            claim: Original claim
            debate_history: List of rounds (R1 + R2)
            early_stopped: Whether debate stopped early
            stop_reason: Reason for early stopping
            evidences: List of gold evidences (not used in prompt)
            r2_anchor: DEPRECATED - no longer used
            
        Returns:
            FinalVerdict
        """
        
        current_round = len(debate_history)
        is_final_round = current_round >= 2
        
        # Build prompt with appropriate inputs
        prompt = self._build_judge_prompt(
            claim=claim,
            debate_history=debate_history,
            model_verdict=model_verdict,
            model_confidence=model_confidence,
            evidences=evidences,
            r2_anchor=r2_anchor
        )
        
        try:
            # Call LLM asynchronously
            response = await self.llm_client.generate_async(
                model=self.config['model'],
                prompt=prompt,
                api_key=self.config['api_key'],
                base_url=self.config['base_url'],
                temperature=self.config.get('temperature', 0.3),
                max_tokens=self.config.get('max_tokens', 1000)
            )
            
            # Parse response
            verdict = self._parse_judge_response(
                response,
                claim,
                evidences,
                debate_history,
                early_stopped,
                stop_reason
            )
            
            logger.info(f"Judge final verdict (async): {verdict.verdict} (conf={verdict.confidence:.2f})")
            
            # NOTE: SAFETY OVERRIDE removed - using SLM-guided signal injection instead
            # The model signal is now part of the prompt context, letting debate decide naturally

            return verdict
            
        except Exception as e:
            logger.error(f"Judge async error: {e}")
            # Fallback: majority voting
            return self._fallback_majority_vote(
                debate_history, 
                early_stopped, 
                stop_reason
            )
